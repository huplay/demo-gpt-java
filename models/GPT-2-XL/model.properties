name = GPT-2 XL (1558M)
source = OpenAI

tokenizer = GPT-2
token.count = 50257
end.of.text.token = 50256
max.length = 1024

hidden.size = 1600
decoder.count = 48
attention.head.count = 25
attention.dividend = 8

attention.type.1 = global
attention.type.2 = global
attention.type.3 = global
attention.type.4 = global
attention.type.5 = global
attention.type.6 = global
attention.type.7 = global
attention.type.8 = global
attention.type.9 = global
attention.type.10 = global
attention.type.11 = global
attention.type.12 = global
attention.type.13 = global
attention.type.14 = global
attention.type.15 = global
attention.type.16 = global
attention.type.17 = global
attention.type.18 = global
attention.type.19 = global
attention.type.20 = global
attention.type.21 = global
attention.type.22 = global
attention.type.23 = global
attention.type.24 = global
attention.type.25 = global
attention.type.26 = global
attention.type.27 = global
attention.type.28 = global
attention.type.29 = global
attention.type.30 = global
attention.type.31 = global
attention.type.32 = global
attention.type.33 = global
attention.type.34 = global
attention.type.35 = global
attention.type.36 = global
attention.type.37 = global
attention.type.38 = global
attention.type.39 = global
attention.type.40 = global
attention.type.41 = global
attention.type.42 = global
attention.type.43 = global
attention.type.44 = global
attention.type.45 = global
attention.type.46 = global
attention.type.47 = global
attention.type.48 = global

epsilon = 1e-5f

data.type = FLOAT32
byte.order = BIG_ENDIAN

# Every weight file (*.w) is ROW organized
matrix.order.*.w = ROW
