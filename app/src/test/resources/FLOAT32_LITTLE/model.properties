name = Test
source = Test

tokenizer = GPT-2
tokenizer.config = GPT-2
token.count = 50257
end.of.text.token = 50256
max.length = 1024

position.embedding = LEARNED
pre.normalization = true

hidden.size = 768
feedforward.size = 3072
decoder.count = 12
attention.head.count = 12
attention.dividend = 8

attention.type.1 = global
attention.type.2 = global
attention.type.3 = global
attention.type.4 = global
attention.type.5 = global
attention.type.6 = global
attention.type.7 = global
attention.type.8 = global
attention.type.9 = global
attention.type.10 = global
attention.type.11 = global
attention.type.12 = global

epsilon = 1e-5f

data.type = FLOAT32
byte.order = LITTLE_ENDIAN
weights.transposed = false
